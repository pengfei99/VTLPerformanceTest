{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test vtl analytics invocation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, LongType, DecimalType\n",
    "import os\n",
    "from pyspark.sql.functions import lit, rand"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 13:50:58 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/04/12 13:50:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/12 13:50:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|2000|   3|   1|\n",
      "|   A|  XX|2001|   4|   9|\n",
      "|   A|  XX|2002|   7|   5|\n",
      "|   A|  XX|2003|   6|   8|\n",
      "|   A|  YY|2000|   9|   3|\n",
      "|   A|  YY|2001|   5|   4|\n",
      "|   A|  YY|2002|  10|   2|\n",
      "|   A|  YY|2003|   5|   7|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"A\", \"XX\", 2000, 3, 1),\n",
    "    (\"A\", \"XX\", 2001, 4, 9),\n",
    "    (\"A\", \"XX\", 2002, 7, 5),\n",
    "    (\"A\", \"XX\", 2003, 6, 8),\n",
    "    (\"A\", \"YY\", 2000, 9, 3),\n",
    "    (\"A\", \"YY\", 2001, 5, 4),\n",
    "    (\"A\", \"YY\", 2002, 10, 2),\n",
    "    (\"A\", \"YY\", 2003, 5, 7)]\n",
    "\n",
    "schema=StructType([StructField(\"Id_1\",StringType(),True),\n",
    "                   StructField(\"Id_2\",StringType(),True),\n",
    "                   StructField(\"Id_3\",IntegerType(),True),\n",
    "                   StructField(\"Me_1\",IntegerType(),True),\n",
    "                   StructField(\"Me_2\",IntegerType(),True)])\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id_1: string (nullable = true)\n",
      " |-- Id_2: string (nullable = true)\n",
      " |-- Id_3: integer (nullable = true)\n",
      " |-- Me_1: integer (nullable = true)\n",
      " |-- Me_2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VTL window function\n",
    "\n",
    "## 1.1 Rank Example\n",
    "\n",
    "origin df\n",
    "\n",
    "DS_1:\n",
    "|Id_1| Id_2| Id_3| Me_1| Me_2|\n",
    "|----|-----|------|----|-----|\n",
    "|A |XX |2000| 3 |1|\n",
    "|A |XX |2001 |4 |9|\n",
    "|A |XX |2002 |7 |5|\n",
    "|A |XX |2003 |6 |8|\n",
    "|A |YY |2000 |9 |3|\n",
    "|A |YY |2001 |5 |4|\n",
    "|A |YY |2002 |10 |2|\n",
    "|A |YY |2003 |5 |7|\n",
    "\n",
    "\n",
    "DS_r := DS_1 [ calc Me2 := rank ( over ( partition by Id_1 , Id_2 order by Me_1 ) ) ]\n",
    "\n",
    "DS_r\n",
    "|Id_1| Id_2| Id_3| Me_1| Me_2|\n",
    "|----|-----|------|----|-----|\n",
    "|A |XX |2000 |3 |1|\n",
    "|A |XX |2001 |4 |2|\n",
    "|A |XX |2003 |6 |3|\n",
    "|A |XX |2002 |7 |4|\n",
    "|A |YY |2001 |5 |1|\n",
    "|A |YY| 2003 |5 |1|\n",
    "|A |YY |2000 |9 |3|\n",
    "|A |YY |2002 |10 |4|\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  YY|2001|   5|   1|\n",
      "|   A|  YY|2003|   5|   1|\n",
      "|   A|  YY|2000|   9|   3|\n",
      "|   A|  YY|2002|  10|   4|\n",
      "|   A|  XX|2000|   3|   1|\n",
      "|   A|  XX|2001|   4|   2|\n",
      "|   A|  XX|2003|   6|   3|\n",
      "|   A|  XX|2002|   7|   4|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,desc\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Me_1\").asc()]\n",
    "win_name_order=win_name.orderBy(order_col_name)\n",
    "\n",
    "\n",
    "df1 = df.drop(\"Me_2\").withColumn(\"Me_2\", rank().over(win_name_order))\n",
    "df1.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 First() Example\n",
    "\n",
    "DS_r := first_value ( DS_1 over ( partition by Id_1, Id_2 order by Id_3 data points between 1 preceding and 1 following) )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+-----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|first|\n",
      "+----+----+----+----+----+-----+\n",
      "|   A|  YY|2000|   9|   3|    9|\n",
      "|   A|  YY|2001|   5|   4|    9|\n",
      "|   A|  YY|2002|  10|   2|    9|\n",
      "|   A|  YY|2003|   5|   7|    9|\n",
      "|   A|  XX|2000|   3|   1|    3|\n",
      "|   A|  XX|2001|   4|   9|    3|\n",
      "|   A|  XX|2002|   7|   5|    3|\n",
      "|   A|  XX|2003|   6|   8|    3|\n",
      "+----+----+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Id_3\").asc()]\n",
    "win_name_order=win_name.orderBy(order_col_name)\n",
    "\n",
    "df_first=df.withColumn(\"first\",first(\"Me_1\").over(win_name_order))\n",
    "df_first.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Last\n",
    "\n",
    "The spark last does not correspond the specification of VTL. So here we just reverse the order and use first() to simulate last() function. For more details, please visit\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|last|\n",
      "+----+----+----+----+----+----+\n",
      "|   A|  YY|2003|   5|   7|   5|\n",
      "|   A|  YY|2002|  10|   2|   5|\n",
      "|   A|  YY|2001|   5|   4|   5|\n",
      "|   A|  YY|2000|   9|   3|   5|\n",
      "|   A|  XX|2003|   6|   8|   6|\n",
      "|   A|  XX|2002|   7|   5|   6|\n",
      "|   A|  XX|2001|   4|   9|   6|\n",
      "|   A|  XX|2000|   3|   1|   6|\n",
      "+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here we reverse the order of windows to use first to get last\n",
    "order_col_name=[col(\"Id_3\").desc()]\n",
    "win_name_order=win_name.orderBy(order_col_name)\n",
    "\n",
    "df_last=df.withColumn(\"last\",first(\"Me_1\").over(win_name_order))\n",
    "df_last.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Lead()\n",
    "\n",
    "DS_r := lead ( DS_1 , 1 over ( partition by Id_1 , Id_2 order by Id_3 ) )\n",
    "\n",
    "Input: DS_1\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 3 1\n",
    "A XX 1994 4 9\n",
    "A XX 1995 7 5\n",
    "A XX 1996 6 8\n",
    "A YY 1993 9 3\n",
    "A YY 1994 5 4\n",
    "A YY 1995 10 2\n",
    "A YY 1996 2 7\n",
    "```\n",
    "\n",
    "Output : DS_r\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 4 9\n",
    "A XX 1994 7 5\n",
    "A XX 1995 6 8\n",
    "A XX 1996 NULL NULL\n",
    "A YY 1993 5 4\n",
    "A YY 1994 10 2\n",
    "A YY 1995 2 7\n",
    "A YY 1996 NULL NULL\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  YY|2000|   5|   4|\n",
      "|   A|  YY|2001|  10|   2|\n",
      "|   A|  YY|2002|   5|   7|\n",
      "|   A|  YY|2003|null|null|\n",
      "|   A|  XX|2000|   4|   9|\n",
      "|   A|  XX|2001|   7|   5|\n",
      "|   A|  XX|2002|   6|   8|\n",
      "|   A|  XX|2003|null|null|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, lag\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Id_3\").asc()]\n",
    "win_name_order=win_name.orderBy(order_col_name)\n",
    "df_input=df\n",
    "col_names=[\"Me_1\",\"Me_2\"]\n",
    "step=1\n",
    "for col_name in col_names:\n",
    "    lead_col_name=f\"lead_{col_name}\"\n",
    "    df_input=df_input.select(\"*\",lead(col_name,1).over(win_name_order).alias(lead_col_name)).drop(col_name).withColumnRenamed(lead_col_name,col_name)\n",
    "df_input.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 Lag()\n",
    "\n",
    "In the ordered set of Data Points of the current partition, the operator returns the value(s) taken from the Data Point at the specified physical offset prior to the current Data Point.\n",
    "\n",
    "If defaultValue is not specified then the value returned when the offset goes outside the partition is NULL.\n",
    "\n",
    "VTL query: DS_r := lag ( DS_1 , 1 over ( partition by Id_1 , Id_2 order by Id_3 ) ) results in:\n",
    "Input: DS_1\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 3 1\n",
    "A XX 1994 4 9\n",
    "A XX 1995 7 5\n",
    "A XX 1996 6 8\n",
    "A YY 1993 9 3\n",
    "A YY 1994 5 4\n",
    "A YY 1995 10 2\n",
    "A YY 1996 2 7\n",
    "\n",
    "```\n",
    "\n",
    "Output: DS_r\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 NULL NULL\n",
    "A XX 1994 3 1\n",
    "A XX 1995 4 9\n",
    "A XX 1996 7 5\n",
    "A YY 1993 NULL NULL\n",
    "A YY 1994 9 3\n",
    "A YY 1995 5 4\n",
    "A YY 1996 10 2\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  YY|2000|null|null|\n",
      "|   A|  YY|2001|   9|   3|\n",
      "|   A|  YY|2002|   5|   4|\n",
      "|   A|  YY|2003|  10|   2|\n",
      "|   A|  XX|2000|null|null|\n",
      "|   A|  XX|2001|   3|   1|\n",
      "|   A|  XX|2002|   4|   9|\n",
      "|   A|  XX|2003|   7|   5|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, lag\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Id_3\").asc()]\n",
    "win_name_order=win_name.orderBy(order_col_name)\n",
    "df_input=df\n",
    "col_names=[\"Me_1\",\"Me_2\"]\n",
    "step=1\n",
    "for col_name in col_names:\n",
    "    lead_col_name=f\"lead_{col_name}\"\n",
    "    df_input=df_input.select(\"*\",lag(col_name,1).over(win_name_order).alias(lead_col_name)).drop(col_name).withColumnRenamed(lead_col_name,col_name)\n",
    "df_input.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.6 ratio_to_report()\n",
    "\n",
    "The operator returns the ratio between the value of the current Data Point and the sum of the values of the partition which the current Data Point belongs to.\n",
    "\n",
    "ETL example\n",
    "\n",
    "DS_r := ratio_to_report ( DS_1 over ( partition by Id_1, Id_2 ) )\n",
    "\n",
    "Note, here even though the above vtl request does not specify which column we need to apply the ratio_to_report on. The result calculate the ratio_to_report on column, Me_1 and Me_2. Because VTL by default will apply all function without parameter on all column which has property **Measurement**\n",
    "\n",
    "Input DS_1\n",
    "\n",
    "```text\n",
    "\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 2000 3 1\n",
    "A XX 2001 4 3\n",
    "A XX 2002 7 5\n",
    "A XX 2003 6 1\n",
    "A YY 2000 12 0\n",
    "A YY 2001 8 8\n",
    "A YY 2002 6 5\n",
    "A YY 2003 14 -3\n",
    "\n",
    "```\n",
    "\n",
    "output: DS_r\n",
    "\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A YY 2000 0.3 0\n",
    "A YY 2001 0.2 0.8\n",
    "A YY 2002 0.15 0.5\n",
    "A YY 2003 0.35 -0.3\n",
    "A XX 2000 0.15 0,1\n",
    "A XX 2001 0.2 0.3\n",
    "A XX 2002 0.35 0.5\n",
    "A XX 2003 0.3 0.1\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|2000|   3|   1|\n",
      "|   A|  XX|2001|   4|   3|\n",
      "|   A|  XX|2002|   7|   5|\n",
      "|   A|  XX|2003|   6|   1|\n",
      "|   A|  YY|2000|  12|   0|\n",
      "|   A|  YY|2001|   8|   8|\n",
      "|   A|  YY|2002|   6|   5|\n",
      "|   A|  YY|2003|  14|  -3|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1=[\n",
    "    (\"A\", \"XX\", 2001, 4, 3),\n",
    "    (\"A\", \"XX\", 2002, 7, 5),\n",
    "    (\"A\", \"XX\", 2000, 3, 1),\n",
    "    (\"A\", \"XX\", 2003, 6, 1),\n",
    "    (\"A\", \"YY\", 2000, 12, 0),\n",
    "    (\"A\", \"YY\", 2001, 8, 8),\n",
    "    (\"A\", \"YY\", 2002, 6, 5),\n",
    "    (\"A\", \"YY\", 2003, 14, -3)]\n",
    "\n",
    "schema1=StructType([StructField(\"Id_1\",StringType(),True),\n",
    "                   StructField(\"Id_2\",StringType(),True),\n",
    "                   StructField(\"Id_3\",IntegerType(),True),\n",
    "                   StructField(\"Me_1\",IntegerType(),True),\n",
    "                   StructField(\"Me_2\",IntegerType(),True)])\n",
    "\n",
    "df1=spark.createDataFrame(data1, schema1)\n",
    "df1.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+----------+\n",
      "|Id_1|Id_2|Id_3|ratio_Me_1|ratio_Me_2|\n",
      "+----+----+----+----------+----------+\n",
      "|   A|  YY|2000|       0.3|       0.0|\n",
      "|   A|  YY|2001|       0.2|       0.8|\n",
      "|   A|  YY|2002|      0.15|       0.5|\n",
      "|   A|  YY|2003|      0.35|      -0.3|\n",
      "|   A|  XX|2000|      0.15|       0.1|\n",
      "|   A|  XX|2001|       0.2|       0.3|\n",
      "|   A|  XX|2002|      0.35|       0.5|\n",
      "|   A|  XX|2003|       0.3|       0.1|\n",
      "+----+----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Id_3\").asc()]\n",
    "win_name_order=win_name.orderBy(rand(100))\n",
    "\n",
    "col_names=[\"Me_1\",\"Me_2\"]\n",
    "for col_name in col_names:\n",
    "    total_col_name=f\"total_{col_name}\"\n",
    "    df1=df1.withColumn(total_col_name,sum(col_name).over(win_name)).withColumn(f\"ratio_{col_name}\",col(col_name)/col(f\"total_{col_name}\")).drop(total_col_name).drop(col_name).withColumnRenamed(total_col_name,col_name)\n",
    "df1.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}