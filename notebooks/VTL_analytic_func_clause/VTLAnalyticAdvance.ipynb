{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test vtl analytics invocation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, LongType, DecimalType\n",
    "import os\n",
    "from pyspark.sql.functions import lit, rand,col, rank, desc, first, last\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-07-02 10:19:46,263 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|1993|   3| 1.0|\n",
      "|   A|  XX|1994|   4| 9.0|\n",
      "|   A|  XX|1995|   7| 5.0|\n",
      "|   A|  XX|1996|   6| 8.0|\n",
      "|   A|  YY|1993|   9| 3.0|\n",
      "|   A|  YY|1994|   5| 4.0|\n",
      "|   A|  YY|1995|  10| 2.0|\n",
      "|   A|  YY|1996|   2| 7.0|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"A\", \"XX\", 1993, 3, 1.0),\n",
    "    (\"A\", \"XX\", 1994, 4, 9.0),\n",
    "    (\"A\", \"XX\", 1995, 7, 5.0),\n",
    "    (\"A\", \"XX\", 1996, 6, 8.0),\n",
    "    (\"A\", \"YY\", 1993, 9, 3.0),\n",
    "    (\"A\", \"YY\", 1994, 5, 4.0),\n",
    "    (\"A\", \"YY\", 1995, 10, 2.0),\n",
    "    (\"A\", \"YY\", 1996, 2, 7.0)]\n",
    "\n",
    "schema=StructType([StructField(\"Id_1\",StringType(),True),\n",
    "                   StructField(\"Id_2\",StringType(),True),\n",
    "                   StructField(\"Year\",IntegerType(),True),\n",
    "                   StructField(\"Me_1\",IntegerType(),True),\n",
    "                   StructField(\"Me_2\",DoubleType(),True)])\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id_1: string (nullable = true)\n",
      " |-- Id_2: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Me_1: integer (nullable = true)\n",
      " |-- Me_2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. VTL advance window function\n",
    "\n",
    "## 2.1 Rank \n",
    "\n",
    "### 2.1.1 Exp1 : window has partition clause and order by clause\n",
    "\n",
    "\n",
    "```text\n",
    "res := ds1 [calc rank_col:= rank ( over ( partition by Id_1, Id_2 order by Year) )]\n",
    "```\n",
    "\n",
    "**Note: rank function requires rows must be ordered, so we can't apply rank on clause that only have partition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+--------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|rank_col|\n",
      "+----+----+----+----+----+--------+\n",
      "|   A|  XX|2000|   3| 1.0|       1|\n",
      "|   A|  XX|2001|   4| 9.0|       2|\n",
      "|   A|  XX|2002|   7| 5.0|       3|\n",
      "|   A|  XX|2003|   6| 8.0|       4|\n",
      "|   A|  YY|2000|   9| 3.0|       1|\n",
      "|   A|  YY|2001|   5| 4.0|       2|\n",
      "|   A|  YY|2002|  10| 2.0|       3|\n",
      "|   A|  YY|2003|   2| 7.0|       4|\n",
      "+----+----+----+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "order_col_names=[col(\"Year\").asc()]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names)\n",
    "\n",
    "new_col_name=f\"rank_col\"\n",
    "\n",
    "\n",
    "df_rank=df.withColumn(new_col_name,rank().over(win_name)) \n",
    "df_rank.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Exp2 : window has partition clause and order by clause with desc\n",
    "\n",
    "\n",
    "```text\n",
    "res := ds1 [calc rank_col:= rank ( over ( partition by Id_1, Id_2 order by Year desc) )]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+--------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|rank_col|\n",
      "+----+----+----+----+----+--------+\n",
      "|   A|  XX|2003|   6| 8.0|       1|\n",
      "|   A|  XX|2002|   7| 5.0|       2|\n",
      "|   A|  XX|2001|   4| 9.0|       3|\n",
      "|   A|  XX|2000|   3| 1.0|       4|\n",
      "|   A|  YY|2003|   2| 7.0|       1|\n",
      "|   A|  YY|2002|  10| 2.0|       2|\n",
      "|   A|  YY|2001|   5| 4.0|       3|\n",
      "|   A|  YY|2000|   9| 3.0|       4|\n",
      "+----+----+----+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "order_col_names=[col(\"Year\").desc()]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names)\n",
    "\n",
    "new_col_name=f\"rank_col\"\n",
    "\n",
    "\n",
    "df_rank=df.withColumn(new_col_name,rank().over(win_name)) \n",
    "df_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Exp3 : window has partition clause, order by clause and data points\n",
    "\n",
    "\n",
    "```text\n",
    "res := ds1 [calc rank_col:= rank ( over ( partition by Id_1, Id_2 order by Year desc data points between unbounded preceding and current data point) )]\n",
    "```\n",
    "\n",
    "**Note rank function can't not take rolling window such as [-1,1]. It requires [unboundedPreceding,currentRow]**. This window specification has no effect, because it's identical to default rolling window definition. So no need to use `data points` and `range between` in rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+--------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|rank_col|\n",
      "+----+----+----+----+----+--------+\n",
      "|   A|  XX|2000|   3| 1.0|       1|\n",
      "|   A|  XX|2001|   4| 9.0|       2|\n",
      "|   A|  XX|2002|   7| 5.0|       3|\n",
      "|   A|  XX|2003|   6| 8.0|       4|\n",
      "|   A|  YY|2000|   9| 3.0|       1|\n",
      "|   A|  YY|2001|   5| 4.0|       2|\n",
      "|   A|  YY|2002|  10| 2.0|       3|\n",
      "|   A|  YY|2003|   5| 7.0|       4|\n",
      "+----+----+----+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "order_col_names=[col(\"Year\")]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names).rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "new_col_name=f\"rank_col\"\n",
    "\n",
    "\n",
    "df_rank=df.withColumn(new_col_name,rank().over(win_name)) \n",
    "df_rank.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 First\n",
    "\n",
    "### 2.2.1 Exp1 : window has partition clause\n",
    "\n",
    "\n",
    "```text\n",
    "res :=  first_value ( ds1 over ( partition by Id_1, Id_2) )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------+----------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|first_Me_1|first_Me_2|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "|   A|  XX|2000|   3| 1.0|         3|       1.0|\n",
      "|   A|  XX|2001|   4| 9.0|         3|       1.0|\n",
      "|   A|  XX|2002|   7| 5.0|         3|       1.0|\n",
      "|   A|  XX|2003|   6| 8.0|         3|       1.0|\n",
      "|   A|  YY|2000|   9| 3.0|         9|       3.0|\n",
      "|   A|  YY|2001|   5| 4.0|         9|       3.0|\n",
      "|   A|  YY|2002|  10| 2.0|         9|       3.0|\n",
      "|   A|  YY|2003|   5| 7.0|         9|       3.0|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"first_{target_col_name1}\"\n",
    "new_col_name2=f\"first_{target_col_name2}\"\n",
    "\n",
    "df_first=df.withColumn(new_col_name1,first(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,first(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_first.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Exp2 : window has partition clause and order by clause\n",
    "\n",
    "vtl query\n",
    "\n",
    "```text\n",
    "res :=  first_value ( ds1 over ( partition by Id_1, Id_2 order by Year desc) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------+----------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|first_Me_1|first_Me_2|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "|   A|  XX|2003|   6| 8.0|         6|       8.0|\n",
      "|   A|  XX|2002|   7| 5.0|         6|       8.0|\n",
      "|   A|  XX|2001|   4| 9.0|         6|       8.0|\n",
      "|   A|  XX|2000|   3| 1.0|         6|       8.0|\n",
      "|   A|  YY|2003|   5| 7.0|         5|       7.0|\n",
      "|   A|  YY|2002|  10| 2.0|         5|       7.0|\n",
      "|   A|  YY|2001|   5| 4.0|         5|       7.0|\n",
      "|   A|  YY|2000|   9| 3.0|         5|       7.0|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "\n",
    "order_col_names=[col(\"Year\").desc()]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"first_{target_col_name1}\"\n",
    "new_col_name2=f\"first_{target_col_name2}\"\n",
    "\n",
    "df_first=df.withColumn(new_col_name1,first(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,first(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_first.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Exp : window has partition, order by and data point\n",
    "\n",
    "The vtl query:\n",
    "\n",
    "```text\n",
    "res := first_value ( ds1 over ( partition by Id_1 order by Id_2 data points between 2 preceding and 2 following) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------+----------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|first_Me_1|first_Me_2|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "|   A|  XX|2000|   3| 1.0|         3|       1.0|\n",
      "|   A|  XX|2001|   4| 9.0|         3|       1.0|\n",
      "|   A|  XX|2002|   7| 5.0|         3|       1.0|\n",
      "|   A|  XX|2003|   6| 8.0|         4|       9.0|\n",
      "|   A|  YY|2000|   9| 3.0|         7|       5.0|\n",
      "|   A|  YY|2001|   5| 4.0|         6|       8.0|\n",
      "|   A|  YY|2002|  10| 2.0|         9|       3.0|\n",
      "|   A|  YY|2003|   5| 7.0|         5|       4.0|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\"]\n",
    "\n",
    "order_col_names=[col(\"Id_2\")]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names).rowsBetween(-2,2)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"first_{target_col_name1}\"\n",
    "new_col_name2=f\"first_{target_col_name2}\"\n",
    "\n",
    "df_first=df.withColumn(new_col_name1,first(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,first(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_first.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Exp : window has partition, order by and range \n",
    "\n",
    "The vtl query:\n",
    "\n",
    "```text\n",
    "res := first_value ( ds1 over ( partition by Id_1 order by Year range between -1 and 1) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------+----------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|first_Me_1|first_Me_2|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "|   A|  XX|2000|   3| 1.0|         3|       1.0|\n",
      "|   A|  YY|2000|   9| 3.0|         3|       1.0|\n",
      "|   A|  XX|2001|   4| 9.0|         3|       1.0|\n",
      "|   A|  YY|2001|   5| 4.0|         3|       1.0|\n",
      "|   A|  XX|2002|   7| 5.0|         4|       9.0|\n",
      "|   A|  YY|2002|  10| 2.0|         4|       9.0|\n",
      "|   A|  XX|2003|   6| 8.0|         7|       5.0|\n",
      "|   A|  YY|2003|   5| 7.0|         7|       5.0|\n",
      "+----+----+----+----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\"]\n",
    "\n",
    "order_col_names=[col(\"Year\")]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names).rangeBetween(-1,1)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"first_{target_col_name1}\"\n",
    "new_col_name2=f\"first_{target_col_name2}\"\n",
    "\n",
    "df_first=df.withColumn(new_col_name1,first(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,first(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_first.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 Last\n",
    "\n",
    "The spark last does not correspond the specification of VTL. So here we just reverse the order and use first() to simulate last() function. For more details, please visit\n",
    "\n",
    "### 2.3.1 Exp1 : window has partition clause\n",
    "\n",
    "\n",
    "```text\n",
    "res :=  last_value ( ds1 over ( partition by Id_1, Id_2) )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+---------+---------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|last_Me_1|last_Me_2|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "|   A|  XX|2000|   3| 1.0|        6|      8.0|\n",
      "|   A|  XX|2001|   4| 9.0|        6|      8.0|\n",
      "|   A|  XX|2002|   7| 5.0|        6|      8.0|\n",
      "|   A|  XX|2003|   6| 8.0|        6|      8.0|\n",
      "|   A|  YY|2000|   9| 3.0|        5|      7.0|\n",
      "|   A|  YY|2001|   5| 4.0|        5|      7.0|\n",
      "|   A|  YY|2002|  10| 2.0|        5|      7.0|\n",
      "|   A|  YY|2003|   5| 7.0|        5|      7.0|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"last_{target_col_name1}\"\n",
    "new_col_name2=f\"last_{target_col_name2}\"\n",
    "\n",
    "df_last=df.withColumn(new_col_name1,last(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,last(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_last.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Exp2 : window has partition clause and order by clause\n",
    "\n",
    "vtl query\n",
    "\n",
    "```text\n",
    "res :=  last_value ( ds1 over ( partition by Id_1, Id_2 order by Year desc) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+---------+---------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|last_Me_1|last_Me_2|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "|   A|  XX|2003|   6| 8.0|        6|      8.0|\n",
      "|   A|  XX|2002|   7| 5.0|        7|      5.0|\n",
      "|   A|  XX|2001|   4| 9.0|        4|      9.0|\n",
      "|   A|  XX|2000|   3| 1.0|        3|      1.0|\n",
      "|   A|  YY|2003|   2| 7.0|        2|      7.0|\n",
      "|   A|  YY|2002|  10| 2.0|       10|      2.0|\n",
      "|   A|  YY|2001|   5| 4.0|        5|      4.0|\n",
      "|   A|  YY|2000|   9| 3.0|        9|      3.0|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "order_col_names=[col(\"Year\").desc()]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"last_{target_col_name1}\"\n",
    "new_col_name2=f\"last_{target_col_name2}\"\n",
    "\n",
    "df_last=df.withColumn(new_col_name1,last(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,last(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_last.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Exp : window has partition, order by and data point\n",
    "\n",
    "The vtl query:\n",
    "\n",
    "```text\n",
    "res := last_value ( ds1 over ( partition by Id_1 order by Id_2 data points between 2 preceding and 2 following) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+---------+---------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|last_Me_1|last_Me_2|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "|   A|  XX|2000|   3| 1.0|        7|      5.0|\n",
      "|   A|  XX|2001|   4| 9.0|        6|      8.0|\n",
      "|   A|  XX|2002|   7| 5.0|        9|      3.0|\n",
      "|   A|  XX|2003|   6| 8.0|        5|      4.0|\n",
      "|   A|  YY|2000|   9| 3.0|       10|      2.0|\n",
      "|   A|  YY|2001|   5| 4.0|        2|      7.0|\n",
      "|   A|  YY|2002|  10| 2.0|        2|      7.0|\n",
      "|   A|  YY|2003|   2| 7.0|        2|      7.0|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\"]\n",
    "order_col_names=[col(\"Id_2\")]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names).rowsBetween(-2,2)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"last_{target_col_name1}\"\n",
    "new_col_name2=f\"last_{target_col_name2}\"\n",
    "\n",
    "df_last=df.withColumn(new_col_name1,last(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,last(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_last.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Exp : window has partition, order by and range \n",
    "\n",
    "The vtl query:\n",
    "\n",
    "```text\n",
    "res := last_value ( ds1 over ( partition by Id_1, Id_2 order by Year range between -1 and 1) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+---------+---------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|last_Me_1|last_Me_2|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "|   A|  XX|1993|   3| 1.0|        4|      9.0|\n",
      "|   A|  XX|1994|   4| 9.0|        7|      5.0|\n",
      "|   A|  XX|1995|   7| 5.0|        6|      8.0|\n",
      "|   A|  XX|1996|   6| 8.0|        6|      8.0|\n",
      "|   A|  YY|1993|   9| 3.0|        5|      4.0|\n",
      "|   A|  YY|1994|   5| 4.0|       10|      2.0|\n",
      "|   A|  YY|1995|  10| 2.0|        2|      7.0|\n",
      "|   A|  YY|1996|   2| 7.0|        2|      7.0|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "order_col_names=[col(\"Year\")]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names).rangeBetween(-1,1)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"last_{target_col_name1}\"\n",
    "new_col_name2=f\"last_{target_col_name2}\"\n",
    "\n",
    "df_last=df.withColumn(new_col_name1,last(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,last(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_last.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Exp to compare with official example\n",
    "\n",
    "\n",
    "```text\n",
    "res :=  = last_value ( ds1 over ( partition by Id_1, Id_2 order by Id_3 data points between 1 preceding and 1 following ) \n",
    "```\n",
    "\n",
    "page 178, the official doc is wrong\n",
    "\n",
    "results in: DS_r\n",
    "\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 4 9\n",
    "A XX 1994 7 9\n",
    "A XX 1995 7 9\n",
    "A XX 1996 7 8\n",
    "A YY 1993 9 4\n",
    "A YY 1994 10 4\n",
    "A YY 1995 10 7\n",
    "A YY 1996 10 7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|1993|   3| 1.0|\n",
      "|   A|  XX|1994|   4| 9.0|\n",
      "|   A|  XX|1995|   7| 5.0|\n",
      "|   A|  XX|1996|   6| 8.0|\n",
      "|   A|  YY|1993|   9| 3.0|\n",
      "|   A|  YY|1994|   5| 4.0|\n",
      "|   A|  YY|1995|  10| 2.0|\n",
      "|   A|  YY|1996|   2| 7.0|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1=[(\"A\", \"XX\", 1993, 3, 1.0),\n",
    "    (\"A\", \"XX\", 1994, 4, 9.0),\n",
    "    (\"A\", \"XX\", 1995, 7, 5.0),\n",
    "    (\"A\", \"XX\", 1996, 6, 8.0),\n",
    "    (\"A\", \"YY\", 1993, 9, 3.0),\n",
    "    (\"A\", \"YY\", 1994, 5, 4.0),\n",
    "    (\"A\", \"YY\", 1995, 10, 2.0),\n",
    "    (\"A\", \"YY\", 1996, 2, 7.0)]\n",
    "\n",
    "schema1=StructType([StructField(\"Id_1\",StringType(),True),\n",
    "                   StructField(\"Id_2\",StringType(),True),\n",
    "                   StructField(\"Year\",IntegerType(),True),\n",
    "                   StructField(\"Me_1\",IntegerType(),True),\n",
    "                   StructField(\"Me_2\",DoubleType(),True)])\n",
    "\n",
    "df1=spark.createDataFrame(data1, schema1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+---------+---------+\n",
      "|Id_1|Id_2|Year|Me_1|Me_2|last_Me_1|last_Me_2|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "|   A|  XX|1993|   3| 1.0|        4|      9.0|\n",
      "|   A|  XX|1994|   4| 9.0|        7|      5.0|\n",
      "|   A|  XX|1995|   7| 5.0|        6|      8.0|\n",
      "|   A|  XX|1996|   6| 8.0|        6|      8.0|\n",
      "|   A|  YY|1993|   9| 3.0|        5|      4.0|\n",
      "|   A|  YY|1994|   5| 4.0|       10|      2.0|\n",
      "|   A|  YY|1995|  10| 2.0|        2|      7.0|\n",
      "|   A|  YY|1996|   2| 7.0|        2|      7.0|\n",
      "+----+----+----+----+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "\n",
    "order_col_names=[col(\"Year\")]\n",
    "\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(order_col_names).rowsBetween(-1,1)\n",
    "\n",
    "target_col_name1=\"Me_1\"\n",
    "target_col_name2=\"Me_2\"\n",
    "new_col_name1=f\"last_{target_col_name1}\"\n",
    "new_col_name2=f\"last_{target_col_name2}\"\n",
    "\n",
    "df_last1=df1.withColumn(new_col_name1,last(target_col_name1).over(win_name))\\\n",
    "           .withColumn(new_col_name2,last(target_col_name2).over(win_name))\n",
    "\n",
    "\n",
    "df_last1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4 Lead()\n",
    "\n",
    "DS_r := lead ( DS_1 , 1 over ( partition by Id_1 , Id_2 order by Id_3 ) )\n",
    "\n",
    "Input: DS_1\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 3 1\n",
    "A XX 1994 4 9\n",
    "A XX 1995 7 5\n",
    "A XX 1996 6 8\n",
    "A YY 1993 9 3\n",
    "A YY 1994 5 4\n",
    "A YY 1995 10 2\n",
    "A YY 1996 2 7\n",
    "```\n",
    "\n",
    "Output : DS_r\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 4 9\n",
    "A XX 1994 7 5\n",
    "A XX 1995 6 8\n",
    "A XX 1996 NULL NULL\n",
    "A YY 1993 5 4\n",
    "A YY 1994 10 2\n",
    "A YY 1995 2 7\n",
    "A YY 1996 NULL NULL\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  YY|2000|   5|   4|\n",
      "|   A|  YY|2001|  10|   2|\n",
      "|   A|  YY|2002|   5|   7|\n",
      "|   A|  YY|2003|null|null|\n",
      "|   A|  XX|2000|   4|   9|\n",
      "|   A|  XX|2001|   7|   5|\n",
      "|   A|  XX|2002|   6|   8|\n",
      "|   A|  XX|2003|null|null|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, lag\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Id_3\").asc()]\n",
    "win_name_order=win_name.orderBy(order_col_name)\n",
    "df_input=df\n",
    "col_names=[\"Me_1\",\"Me_2\"]\n",
    "step=1\n",
    "for col_name in col_names:\n",
    "    lead_col_name=f\"lead_{col_name}\"\n",
    "    df_input=df_input.select(\"*\",lead(col_name,1).over(win_name_order).alias(lead_col_name)).drop(col_name).withColumnRenamed(lead_col_name,col_name)\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.5 Lag()\n",
    "\n",
    "In the ordered set of Data Points of the current partition, the operator returns the value(s) taken from the Data Point at the specified physical offset prior to the current Data Point.\n",
    "\n",
    "If defaultValue is not specified then the value returned when the offset goes outside the partition is NULL.\n",
    "\n",
    "VTL query: DS_r := lag ( DS_1 , 1 over ( partition by Id_1 , Id_2 order by Id_3 ) ) results in:\n",
    "Input: DS_1\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 3 1\n",
    "A XX 1994 4 9\n",
    "A XX 1995 7 5\n",
    "A XX 1996 6 8\n",
    "A YY 1993 9 3\n",
    "A YY 1994 5 4\n",
    "A YY 1995 10 2\n",
    "A YY 1996 2 7\n",
    "\n",
    "```\n",
    "\n",
    "Output: DS_r\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 1993 NULL NULL\n",
    "A XX 1994 3 1\n",
    "A XX 1995 4 9\n",
    "A XX 1996 7 5\n",
    "A YY 1993 NULL NULL\n",
    "A YY 1994 9 3\n",
    "A YY 1995 5 4\n",
    "A YY 1996 10 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  YY|2000|null|null|\n",
      "|   A|  YY|2001|   9|   3|\n",
      "|   A|  YY|2002|   5|   4|\n",
      "|   A|  YY|2003|  10|   2|\n",
      "|   A|  XX|2000|null|null|\n",
      "|   A|  XX|2001|   3|   1|\n",
      "|   A|  XX|2002|   4|   9|\n",
      "|   A|  XX|2003|   7|   5|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, lag\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Id_3\").asc()]\n",
    "win_name_order=win_name.orderBy(order_col_name)\n",
    "df_input=df\n",
    "col_names=[\"Me_1\",\"Me_2\"]\n",
    "step=1\n",
    "for col_name in col_names:\n",
    "    lead_col_name=f\"lead_{col_name}\"\n",
    "    df_input=df_input.select(\"*\",lag(col_name,1).over(win_name_order).alias(lead_col_name)).drop(col_name).withColumnRenamed(lead_col_name,col_name)\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.6 ratio_to_report()\n",
    "\n",
    "The operator returns the ratio between the value of the current Data Point and the sum of the values of the partition which the current Data Point belongs to.\n",
    "\n",
    "ETL example\n",
    "\n",
    "DS_r := ratio_to_report ( DS_1 over ( partition by Id_1, Id_2 ) )\n",
    "\n",
    "Note, here even though the above vtl request does not specify which column we need to apply the ratio_to_report on. The result calculate the ratio_to_report on column, Me_1 and Me_2. Because VTL by default will apply all function without parameter on all column which has property **Measurement**\n",
    "\n",
    "Input DS_1\n",
    "\n",
    "```text\n",
    "\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A XX 2000 3 1\n",
    "A XX 2001 4 3\n",
    "A XX 2002 7 5\n",
    "A XX 2003 6 1\n",
    "A YY 2000 12 0\n",
    "A YY 2001 8 8\n",
    "A YY 2002 6 5\n",
    "A YY 2003 14 -3\n",
    "\n",
    "```\n",
    "\n",
    "output: DS_r\n",
    "\n",
    "```text\n",
    "Id_1 Id_2 Id_3 Me_1 Me_2\n",
    "A YY 2000 0.3 0\n",
    "A YY 2001 0.2 0.8\n",
    "A YY 2002 0.15 0.5\n",
    "A YY 2003 0.35 -0.3\n",
    "A XX 2000 0.15 0,1\n",
    "A XX 2001 0.2 0.3\n",
    "A XX 2002 0.35 0.5\n",
    "A XX 2003 0.3 0.1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|2000|   3|   1|\n",
      "|   A|  XX|2001|   4|   3|\n",
      "|   A|  XX|2002|   7|   5|\n",
      "|   A|  XX|2003|   6|   1|\n",
      "|   A|  YY|2000|  12|   0|\n",
      "|   A|  YY|2001|   8|   8|\n",
      "|   A|  YY|2002|   6|   5|\n",
      "|   A|  YY|2003|  14|  -3|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1=[\n",
    "    (\"A\", \"XX\", 2001, 4, 3),\n",
    "    (\"A\", \"XX\", 2002, 7, 5),\n",
    "    (\"A\", \"XX\", 2000, 3, 1),\n",
    "    (\"A\", \"XX\", 2003, 6, 1),\n",
    "    (\"A\", \"YY\", 2000, 12, 0),\n",
    "    (\"A\", \"YY\", 2001, 8, 8),\n",
    "    (\"A\", \"YY\", 2002, 6, 5),\n",
    "    (\"A\", \"YY\", 2003, 14, -3)]\n",
    "\n",
    "schema1=StructType([StructField(\"Id_1\",StringType(),True),\n",
    "                   StructField(\"Id_2\",StringType(),True),\n",
    "                   StructField(\"Id_3\",IntegerType(),True),\n",
    "                   StructField(\"Me_1\",IntegerType(),True),\n",
    "                   StructField(\"Me_2\",IntegerType(),True)])\n",
    "\n",
    "df1=spark.createDataFrame(data1, schema1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+----------+\n",
      "|Id_1|Id_2|Id_3|ratio_Me_1|ratio_Me_2|\n",
      "+----+----+----+----------+----------+\n",
      "|   A|  YY|2000|       0.3|       0.0|\n",
      "|   A|  YY|2001|       0.2|       0.8|\n",
      "|   A|  YY|2002|      0.15|       0.5|\n",
      "|   A|  YY|2003|      0.35|      -0.3|\n",
      "|   A|  XX|2000|      0.15|       0.1|\n",
      "|   A|  XX|2001|       0.2|       0.3|\n",
      "|   A|  XX|2002|      0.35|       0.5|\n",
      "|   A|  XX|2003|       0.3|       0.1|\n",
      "+----+----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "partition_col_name=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_name)\n",
    "\n",
    "order_col_name=[col(\"Id_3\").asc()]\n",
    "win_name_order=win_name.orderBy(rand(100))\n",
    "\n",
    "col_names=[\"Me_1\",\"Me_2\"]\n",
    "for col_name in col_names:\n",
    "    total_col_name=f\"total_{col_name}\"\n",
    "    df1=df1.withColumn(total_col_name,sum(col_name).over(win_name)).withColumn(f\"ratio_{col_name}\",col(col_name)/col(f\"total_{col_name}\")).drop(total_col_name).drop(col_name).withColumnRenamed(total_col_name,col_name)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
