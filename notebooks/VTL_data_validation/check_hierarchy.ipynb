{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTL Check hierarchy\n",
    "\n",
    "The original doc can be found at **line 6399 of VTL-2.0-Reference-Manual**\n",
    "\n",
    "## Syntax\n",
    "\n",
    "```text\n",
    "check_hierarchy ( op , hr { condition condComp { , condComp }* } { rule ruleComp }\n",
    "{ mode } { input } { output } )\n",
    "  - mode ::= non_null | non_zero | partial_null | partial_zero | always_null | always_zero\n",
    "  - input ::= dataset | dataset_priority\n",
    "  - output ::= invalid | all | all_measures\n",
    "\n",
    "```\n",
    "\n",
    "- **op**: the Data Set to be checked\n",
    "- **hr**: the hierarchical Ruleset to be used\n",
    "- **condComp**: `condComp` is a Component of `op` to be associated (in positional order) to the conditioning Value Domains or Variables defined in `hr` (if any).\n",
    "- **ruleComp**: Component of `op`\n",
    "- **mode**: this parameter specifies how to treat the possible missing Data Points corresponding to the Code Items in the left and right sides of the rules and which Data Points are produced in output. The meaning of the possible values of the parameter is explained below.\n",
    "- **output**: specifies the Data Points and the Measures of the resulting Data Set:\n",
    "     - **invalid**: the resulting Data Set contains a Data Point for each Data Point of `op` and  each Rule in `dpr` that evaluates to `FALSE` on that Data Point. The resulting Data Set has the Measures of op.\n",
    "     - **all**: the resulting Data Set contains a data point for each Data Point of `op` and each Rule in `dpr`. The resulting Data Set has the boolean Measure bool_var.\n",
    "     - **all_measures**: the resulting Data Set contains a Data Point for each Data Point of `op` and each Rule in `dpr`. The resulting dataset has the Measures of `op` and the  boolean Measure bool_var.\n",
    "     - If not specified then output is assumed to be invalid. See the Behaviour for further details.\n",
    "\n",
    "## Example\n",
    "\n",
    "**Setp 1: Define hierarchical ruleset.**\n",
    "\n",
    "```text\n",
    "define hierarchical ruleset HR_1 ( valuedomain rule VD_1 ) is\n",
    "   R010 : A = J + K + L errorcode Bad_val errorlevel 5;\n",
    "   R020 : B = M + N + O errorcode Bad_val errorlevel 5;\n",
    "   R070 : G = B + C errorcode Bad_val errorlevel 1\n",
    "```\n",
    "\n",
    "**Step 2: Apply ruleset to a dataset**\n",
    "\n",
    "```text\n",
    "DS_r := check_hierarchy ( DS_1, HR_1 rule Id_2 partial_null all )\n",
    "````\n",
    "\n",
    "Given a dataset ds_1:\n",
    "\n",
    "```text\n",
    "Id_1,Id_2,Me_1\n",
    "2010,A,5\n",
    "2010,B,11\n",
    "2010,C,0\n",
    "2010,G,19\n",
    "2010,H,NULL\n",
    "2010,I,14\n",
    "2010,M,2\n",
    "2010,N,5\n",
    "2010,O,4\n",
    "2010,P,7\n",
    "2010,Q,-7\n",
    "2010,S,3\n",
    "2010,T,9\n",
    "2010,U,NULL\n",
    "2010,V,6\n",
    "```\n",
    "\n",
    "The output should be:\n",
    "\n",
    "```text\n",
    "\n",
    "Id_1,Id_2,ruleid,Bool_var,imbalance,errorcode,errorlevel\n",
    "2010,A,R010,NULL,NULL,NULL,NULL\n",
    "2010,B,R020,TRUE,0,NULL,NULL\n",
    "2010,G,R070,FALSE,8,Bad_val,1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import col, lit, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"VTLValidation_check_hierarchy\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"VTLValidation_check_hierarchy\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Id_1|Id_2|Me_1|\n",
      "+----+----+----+\n",
      "|2010|   A|   5|\n",
      "|2010|   B|  11|\n",
      "|2010|   C|   0|\n",
      "|2010|   G|  19|\n",
      "|2010|   H|NULL|\n",
      "|2010|   I|  14|\n",
      "|2010|   M|   2|\n",
      "|2010|   N|   5|\n",
      "|2010|   O|   4|\n",
      "|2010|   P|   7|\n",
      "|2010|   Q|  -7|\n",
      "|2010|   S|   3|\n",
      "|2010|   T|   9|\n",
      "|2010|   U|NULL|\n",
      "|2010|   V|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_path=\"../../data\"\n",
    "data_path=f\"{root_path}/check_hier_ds.csv\"\n",
    "\n",
    "df=spark.read.csv(data_path, header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Implement hierarchical ruleset\n",
    "\n",
    "A hierarchical ruleset can contain one or more rules. For each rule, we need to define a corresponding validation function in spark that implements the logic and generate the resulting columns.\n",
    "\n",
    "Note it has 3 modes (e.g. invalid, all, all_measures), and each mode has a unique output column formats. So each generated function must take consideration of that.\n",
    "\n",
    "Below functions should be generated when we encounter **define datapoint ruleset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# this function get the value of each operant\n",
    "def get_op_var(ds,reference_col,val_col,op_val):\n",
    "    cell=ds.filter(col(reference_col)==op_val).select(val_col).collect()\n",
    "    if len(cell) and len(cell[0]):\n",
    "        return cell[0][0]\n",
    "    else:\n",
    "        raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# this function is for complete the output column of check option : all\n",
    "def trans_for_all(ds,rule_id,error_code,error_level):\n",
    "    return ds.withColumn(\"rule_id\",lit(rule_id)) \\\n",
    "       .withColumn(\"error_code\",when(col(\"bool_var\")==False,error_code)) \\\n",
    "        .withColumn(\"error_level\",when(col(\"bool_var\")==False,error_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# implementation of rule R010 in HR_1, this should be generated based on HR_1:R010 definition\n",
    "# A = J + K + L\n",
    "def r010(ds,option):\n",
    "    rule_id=\"R010\"\n",
    "    reference_col=\"Id_2\"\n",
    "    val_col=\"Me_1\"\n",
    "    op1=\"A\"\n",
    "    op2=\"J\"\n",
    "    op3=\"K\"\n",
    "    op4=\"L\"\n",
    "\n",
    "    # get the value of op1\n",
    "    try:\n",
    "        val1= get_op_var(df,reference_col,val_col,op1)\n",
    "        val2=get_op_var(df,reference_col,val_col,op2)\n",
    "        val3=get_op_var(df,reference_col,val_col,op3)\n",
    "        val4=get_op_var(df,reference_col,val_col,op4)\n",
    "        print(f\"val1: {val1}, val2: {val2}, val3: {val3}, val4 : {val4}\")\n",
    "        imbalance=val1-val2-val3-val4\n",
    "    except TypeError:\n",
    "        imbalance=None\n",
    "    error_code=\"Bad credit\"\n",
    "    error_level=\"5\"\n",
    "    if option==\"invalid\":\n",
    "        tmp=ds.filter(col(val_col)==op1).withColumn(\"imbalance\",lit(imbalance)).withColumn(\"bool_var\", when(col(imbalance)==0,True).when(col(imbalance) is None, None).otherwise(False))\n",
    "        return tmp\n",
    "    elif option==\"all\":\n",
    "\n",
    "        return 1\n",
    "    elif option==\"all_measures\":\n",
    "        return ds\n",
    "    else:\n",
    "        raise ValueError(\"Unknown option value, accepted values are : invalid, all, all_measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.col.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.Column.<init>(Column.scala:146)\n\tat org.apache.spark.sql.Column$.apply(Column.scala:37)\n\tat org.apache.spark.sql.functions$.col(functions.scala:96)\n\tat org.apache.spark.sql.functions.col(functions.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r010_resu\u001b[38;5;241m=\u001b[39m\u001b[43mr010\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [30], line 25\u001b[0m, in \u001b[0;36mr010\u001b[0;34m(ds, option)\u001b[0m\n\u001b[1;32m     23\u001b[0m error_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m option\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     tmp\u001b[38;5;241m=\u001b[39mds\u001b[38;5;241m.\u001b[39mfilter(col(val_col)\u001b[38;5;241m==\u001b[39mop1)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimbalance\u001b[39m\u001b[38;5;124m\"\u001b[39m,lit(imbalance))\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool_var\u001b[39m\u001b[38;5;124m\"\u001b[39m, when(\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimbalance\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mwhen(col(imbalance) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/pyspark/sql/functions.py:151\u001b[0m, in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m1.3\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcol\u001b[39m(col: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.'\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    Column<'x'>\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/pyspark/sql/functions.py:85\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     84\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.col.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.Column.<init>(Column.scala:146)\n\tat org.apache.spark.sql.Column$.apply(Column.scala:37)\n\tat org.apache.spark.sql.functions$.col(functions.scala:96)\n\tat org.apache.spark.sql.functions.col(functions.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "r010_resu=r010(df,\"invalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r010_resu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mr010_resu\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r010_resu' is not defined"
     ]
    }
   ],
   "source": [
    "r010_resu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# implementation of rule R020 in HR_1, this should be generated based on HR_1:R010 definition\n",
    "# B = M + N + O\n",
    "def r020(ds,option):\n",
    "    rule_id=\"R020\"\n",
    "    reference_col=\"Id_2\"\n",
    "    val_col=\"Me_1\"\n",
    "    op1=\"B\"\n",
    "    op2=\"M\"\n",
    "    op3=\"N\"\n",
    "    op4=\"O\"\n",
    "\n",
    "    # get the value of op1\n",
    "    try:\n",
    "        val1= get_op_var(df,reference_col,val_col,op1)\n",
    "        val2=get_op_var(df,reference_col,val_col,op2)\n",
    "        val3=get_op_var(df,reference_col,val_col,op3)\n",
    "        val4=get_op_var(df,reference_col,val_col,op4)\n",
    "        print(f\"val1: {val1}, val2: {val2}, val3: {val3}, val4 : {val4}\")\n",
    "        imbalance=val1-val2-val3-val4\n",
    "        print(imbalance)\n",
    "    except TypeError:\n",
    "        imbalance=None\n",
    "    error_code=\"Bad credit\"\n",
    "    error_level=\"5\"\n",
    "    if option==\"invalid\":\n",
    "        tmp=ds.filter(col(val_col)==op1).withColumn(\"imbalance\",lit(imbalance))\n",
    "        tmp=tmp.withColumn(\"bool_var\", when(col(imbalance)==0,True).when(col(imbalance) is None, None).otherwise(False))\n",
    "        return tmp\n",
    "    elif option==\"all\":\n",
    "\n",
    "        return 1\n",
    "    elif option==\"all_measures\":\n",
    "        return ds\n",
    "    else:\n",
    "        raise ValueError(\"Unknown option value, accepted values are : invalid, all, all_measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val1: 11, val2: 2, val3: 5, val4 : 4\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.col.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.Column.<init>(Column.scala:146)\n\tat org.apache.spark.sql.Column$.apply(Column.scala:37)\n\tat org.apache.spark.sql.functions$.col(functions.scala:96)\n\tat org.apache.spark.sql.functions.col(functions.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r020_resu\u001b[38;5;241m=\u001b[39m\u001b[43mr020\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [40], line 27\u001b[0m, in \u001b[0;36mr020\u001b[0;34m(ds, option)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m option\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     26\u001b[0m     tmp\u001b[38;5;241m=\u001b[39mds\u001b[38;5;241m.\u001b[39mfilter(col(val_col)\u001b[38;5;241m==\u001b[39mop1)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimbalance\u001b[39m\u001b[38;5;124m\"\u001b[39m,lit(imbalance))\n\u001b[0;32m---> 27\u001b[0m     tmp\u001b[38;5;241m=\u001b[39mtmp\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool_var\u001b[39m\u001b[38;5;124m\"\u001b[39m, when(\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimbalance\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mwhen(col(imbalance) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/pyspark/sql/functions.py:151\u001b[0m, in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m1.3\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcol\u001b[39m(col: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.'\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    Column<'x'>\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/pyspark/sql/functions.py:85\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     84\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vtlperformancetest-J2H70pBo-py3.8/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.col.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.Column.<init>(Column.scala:146)\n\tat org.apache.spark.sql.Column$.apply(Column.scala:37)\n\tat org.apache.spark.sql.functions$.col(functions.scala:96)\n\tat org.apache.spark.sql.functions.col(functions.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "r020_resu=r020(df,\"invalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mr020_resu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "r020_resu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----+--------+-------+----------+-----------+\n",
      "|Id_1|Id_2|  Id_3|Me_1|bool_var|rule_id|error_code|error_level|\n",
      "+----+----+------+----+--------+-------+----------+-----------+\n",
      "|2011|   1|CREDIT|  10|    true| dpr1_1|      null|       null|\n",
      "|2011|   1| DEBIT|  -2|    true| dpr1_1|      null|       null|\n",
      "|2012|   1|CREDIT|  10|    true| dpr1_1|      null|       null|\n",
      "|2012|   1| DEBIT|   2|    true| dpr1_1|      null|       null|\n",
      "|2011|   1|CREDIT|  10|    true| dpr1_2|      null|       null|\n",
      "|2011|   1| DEBIT|  -2|   false| dpr1_2| Bad debit|          6|\n",
      "|2012|   1|CREDIT|  10|    true| dpr1_2|      null|       null|\n",
      "|2012|   1| DEBIT|   2|    true| dpr1_2|      null|       null|\n",
      "+----+----+------+----+--------+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dpr1_1_resu.union(dpr1_2_resu).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply data point ruleset on a data frame\n",
    "\n",
    "This function should be generated when a function **check_datapoint**,\n",
    "note the rule sets and rules are generated in step 1. They must be present when we call **check_datapoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def data_validation(ds,rules,option):\n",
    "    result=rules[0](ds,option)\n",
    "    for i in range(1,len(rules)):\n",
    "        result=result.union(rules[i](ds,option))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rule_sets=[dpr1_1,dpr1_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "invalid_resu=data_validation(df,rule_sets,\"invalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---------+-------+----------+-----------+\n",
      "|Id_1|Id_2| Id_3|obs_value|rule_id|error_code|error_level|\n",
      "+----+----+-----+---------+-------+----------+-----------+\n",
      "|2011|   1|DEBIT|       -2| dpr1_2| Bad debit|          6|\n",
      "+----+----+-----+---------+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invalid_resu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_resu=data_validation(df,rule_sets,\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----+--------+-------+----------+-----------+\n",
      "|Id_1|Id_2|  Id_3|Me_1|bool_var|rule_id|error_code|error_level|\n",
      "+----+----+------+----+--------+-------+----------+-----------+\n",
      "|2011|   1|CREDIT|  10|    true| dpr1_1|      null|       null|\n",
      "|2011|   1| DEBIT|  -2|    true| dpr1_1|      null|       null|\n",
      "|2012|   1|CREDIT|  10|    true| dpr1_1|      null|       null|\n",
      "|2012|   1| DEBIT|   2|    true| dpr1_1|      null|       null|\n",
      "|2011|   1|CREDIT|  10|    true| dpr1_2|      null|       null|\n",
      "|2011|   1| DEBIT|  -2|   false| dpr1_2| Bad debit|          6|\n",
      "|2012|   1|CREDIT|  10|    true| dpr1_2|      null|       null|\n",
      "|2012|   1| DEBIT|   2|    true| dpr1_2|      null|       null|\n",
      "+----+----+------+----+--------+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_resu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
