{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pivot\n",
    "\n",
    "Given the Data Set DS_1:  (Line 7150)\n",
    "\n",
    "```text\n",
    "+----+----+----+----+\n",
    "|Id_1|Id_2|Me_1|At_1|\n",
    "+----+----+----+----+\n",
    "|   1|   A|   5|   E|\n",
    "|   1|   B|   2|   F|\n",
    "|   1|   C|   7|   F|\n",
    "|   2|   A|   3|   E|\n",
    "|   2|   B|   4|   E|\n",
    "|   2|   C|   9|   F|\n",
    "+----+----+----+----+\n",
    "```\n",
    "\n",
    "Example1: DS_r := Ds_1 [ pivot Id_2, Me_1 ] results in:\n",
    "\n",
    "```text\n",
    "+----+---+---+---+\n",
    "|Id_1|  A|  B|  C|\n",
    "+----+---+---+---+\n",
    "|   1|  5|  2|  7|\n",
    "|   2|  3|  4|  9|\n",
    "+----+---+---+---+\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/11 16:37:51 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/05/11 16:37:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/11 16:37:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"VTLPIVOT\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"VTLPIVOT\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|Id_1|Id_2|Me_1|At_1|\n",
      "+----+----+----+----+\n",
      "|   1|   A|   5|   E|\n",
      "|   1|   B|   2|   F|\n",
      "|   1|   C|   7|   F|\n",
      "|   2|   A|   3|   E|\n",
      "|   2|   B|   4|   E|\n",
      "|   2|   C|   9|   F|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_path=\"../../data\"\n",
    "data_path=f\"{root_path}/pivot_ds1.csv\"\n",
    "\n",
    "df=spark.read.csv(data_path, header=True,inferSchema=True)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id_1: integer (nullable = true)\n",
      " |-- Id_2: string (nullable = true)\n",
      " |-- Me_1: integer (nullable = true)\n",
      " |-- At_1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# DS_r := Ds_1 [ pivot Id_2, Me_1 ]\n",
    "identifier =\"Id_2\"\n",
    "measure = \"Me_1\"\n",
    "df_resu = df.groupby(\"Id_1\").pivot(identifier).sum(measure)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> We need one more argument to make it work. For example in this example there is a group by of column \"Id_1\" before the pivot, which is not specified in the `DS_r := Ds_1 [ pivot Id_2, Me_1 ]`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+\n",
      "|Id_1|  A|  B|  C|\n",
      "+----+---+---+---+\n",
      "|   1|  5|  2|  7|\n",
      "|   2|  3|  4|  9|\n",
      "+----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_resu.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df_test = spark.createDataFrame(data = data, schema = columns)\n",
    "df_test.printSchema()\n",
    "df_test.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DS_r := Ds_1 [ pivot Country, Amount ]\n",
    "identifier =\"Country\"\n",
    "measure = \"Amount\"\n",
    "\n",
    "pivotDF = df_test.groupBy(\"Product\").pivot(identifier).sum(measure)\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unpivot\n",
    "\n",
    "Given the Data Set DS_1:\n",
    "\n",
    "```text\n",
    "+----+---+---+---+\n",
    "|Id_1|  A|  B|  C|\n",
    "+----+---+---+---+\n",
    "|   1|  5|  2|  7|\n",
    "|   2|  3|  4|  9|\n",
    "+----+---+---+---+\n",
    "```\n",
    "\n",
    "DS_r := DS_1 [ unpivot Id_2, Me_1] results in:\n",
    "\n",
    "```text\n",
    "+----+----+----+\n",
    "|Id_1|Id_2|Me_1|\n",
    "+----+----+----+\n",
    "|1   |A   |5   |\n",
    "|1   |B   |2   |\n",
    "|1   |C   |7   |\n",
    "|2   |A   |3   |\n",
    "|2   |B   |4   |\n",
    "|2   |C   |9   |\n",
    "+----+----+----+\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Id_1|Id_2|Me_1|\n",
      "+----+----+----+\n",
      "|1   |A   |5   |\n",
      "|1   |B   |2   |\n",
      "|1   |C   |7   |\n",
      "|2   |A   |3   |\n",
      "|2   |B   |4   |\n",
      "|2   |C   |9   |\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivotExpr = \"stack(3, 'A', A, 'B', B, 'C', C) as (Id_2,Me_1)\"\n",
    "unPivotDF = df_resu.select(\"Id_1\", expr(unpivotExpr)).where(\"Me_1 is not null\")\n",
    "unPivotDF.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> For unpivot, the same problem, we don't have information of which column is the unchanged column, and which columns are the columns which needs to be unpivot. For example the input table has four columns (Id_1,A,B,C), the `DS_r := DS_1 [ unpivot Id_2, Me_1]` only defined the column name of the target dataset, how can I know that column A, B, C is the column that we need to unpivot?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
