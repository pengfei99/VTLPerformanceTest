{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# VTL Simple analytic function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, LongType, DecimalType\n",
    "import os\n",
    "from pyspark.sql.functions import lit, count,sum,avg,first"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/13 15:11:18 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/04/13 15:11:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/13 15:11:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/13 15:11:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|2000|   3|   1|\n",
      "|   A|  XX|2011|   4|   9|\n",
      "|   A|  XX|2022|   7|   5|\n",
      "|   A|  XX|2023|   6|   8|\n",
      "|   A|  YY|2000|   9|   3|\n",
      "|   A|  YY|2011|   5|   4|\n",
      "|   A|  YY|2022|  10|   2|\n",
      "|   A|  YY|2023|   5|   7|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"A\", \"XX\", 2000, 3, 1),\n",
    "    (\"A\", \"XX\", 2011, 4, 9),\n",
    "    (\"A\", \"XX\", 2022, 7, 5),\n",
    "    (\"A\", \"XX\", 2023, 6, 8),\n",
    "    (\"A\", \"YY\", 2000, 9, 3),\n",
    "    (\"A\", \"YY\", 2011, 5, 4),\n",
    "    (\"A\", \"YY\", 2022, 10, 2),\n",
    "    (\"A\", \"YY\", 2023, 5, 7)]\n",
    "\n",
    "schema=StructType([StructField(\"Id_1\",StringType(),True),\n",
    "                   StructField(\"Id_2\",StringType(),True),\n",
    "                   StructField(\"Id_3\",IntegerType(),True),\n",
    "                   StructField(\"Me_1\",IntegerType(),True),\n",
    "                   StructField(\"Me_2\",IntegerType(),True)])\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VTL WindowClause\n",
    "\n",
    "windowClause ::= { data points | range } between limitClause and limitClausecount ( DS_1 over ( partition by Id_1 ) )\n",
    "\n",
    "It specifies how to apply a sliding window on the ordered Data Points. The keyword **data points** means that the sliding window includes a certain number of Data Points before and after the current Data Point in the order given by the orderClause. The keyword **range** means that the sliding windows includes all the Data Points whose values are in a certain range in respect to the value, for the current Data Point, of the Measure which the analytic is applied to.\n",
    "\n",
    "## data points example\n",
    "\n",
    "\n",
    "DS_r := first_value ( DS_1 over ( partition by Id_1, Id_2 order by Id_3 data points between 1 preceding and 1 following) )\n",
    "\n",
    "The current row index is 0. The window frame is 1 preceding, and 1 following, so the window always contains three rows with index [-1,0,1] at most. Note that the frame could not overwrite the window of the partition. For example, for the last row of one partition, you will only have two rows [-1,0], because current row is the last row, and the next row is in another partition.  For the first row of one partition it's the same, you will only have two rows [1,0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------------+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|first_value_Me_1|\n",
      "+----+----+----+----+----+----------------+\n",
      "|   A|  YY|2000|   9|   3|               9|\n",
      "|   A|  YY|2011|   5|   4|               9|\n",
      "|   A|  YY|2022|  10|   2|               5|\n",
      "|   A|  YY|2023|   5|   7|              10|\n",
      "|   A|  XX|2000|   3|   1|               3|\n",
      "|   A|  XX|2011|   4|   9|               3|\n",
      "|   A|  XX|2022|   7|   5|               4|\n",
      "|   A|  XX|2023|   6|   8|               7|\n",
      "+----+----+----+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(\"Id_3\").rowsBetween(-1,1)\n",
    "target_col_name=\"Me_1\"\n",
    "new_col_name=f\"first_value_{target_col_name}\"\n",
    "df_collect=df.withColumn(new_col_name,first(target_col_name).over(win_name))\n",
    "df_collect.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Range example\n",
    "DS_r := first_value ( DS_1 over ( partition by Id_1, Id_2 order by Id_3 range between 1 preceding and 1 following) )\n",
    "\n",
    "The range use the value of the order by column of the current row as baseline. For example, with the above query, the order by column is **Id_3**.\n",
    "As a result, the baseline column is **Id_3**. With range between 1 preceding and 1 following, we now use the value of current row of column **Id_3** to build the window. For the first row, the value of **Id_3** is 2000, so the window range is 2000-1=1999 and 2000+1=2001."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------------+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|first_value_Me_1|\n",
      "+----+----+----+----+----+----------------+\n",
      "|   A|  YY|2000|   9|   3|               9|\n",
      "|   A|  YY|2011|   5|   4|               5|\n",
      "|   A|  YY|2022|  10|   2|              10|\n",
      "|   A|  YY|2023|   5|   7|              10|\n",
      "|   A|  XX|2000|   3|   1|               3|\n",
      "|   A|  XX|2011|   4|   9|               4|\n",
      "|   A|  XX|2022|   7|   5|               7|\n",
      "|   A|  XX|2023|   6|   8|               7|\n",
      "+----+----+----+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(\"Id_3\").rangeBetween(-1,1)\n",
    "target_col_name=\"Me_1\"\n",
    "new_col_name=f\"first_value_{target_col_name}\"\n",
    "df_collect=df.withColumn(new_col_name,first(target_col_name).over(win_name))\n",
    "df_collect.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}