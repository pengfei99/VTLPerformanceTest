{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# VTL Simple analytic function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, LongType, DecimalType\n",
    "import os\n",
    "from pyspark.sql.functions import lit, count,sum,avg,first, var_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"VTLAnalytic\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|2000|   3|   1|\n",
      "|   A|  XX|2001|   4|   9|\n",
      "|   A|  XX|2002|   7|   5|\n",
      "|   A|  XX|2003|   6|   8|\n",
      "|   A|  YY|2000|   9|   3|\n",
      "|   A|  YY|2001|   5|   4|\n",
      "|   A|  YY|2002|  10|   2|\n",
      "|   A|  YY|2003|   5|   7|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"A\", \"XX\", 2000, 3, 1),\n",
    "    (\"A\", \"XX\", 2001, 4, 9),\n",
    "    (\"A\", \"XX\", 2002, 7, 5),\n",
    "    (\"A\", \"XX\", 2003, 6, 8),\n",
    "    (\"A\", \"YY\", 2000, 9, 3),\n",
    "    (\"A\", \"YY\", 2001, 5, 4),\n",
    "    (\"A\", \"YY\", 2002, 10, 2),\n",
    "    (\"A\", \"YY\", 2003, 5, 7)]\n",
    "\n",
    "schema=StructType([StructField(\"Id_1\",StringType(),True),\n",
    "                   StructField(\"Id_2\",StringType(),True),\n",
    "                   StructField(\"Id_3\",IntegerType(),True),\n",
    "                   StructField(\"Me_1\",IntegerType(),True),\n",
    "                   StructField(\"Me_2\",IntegerType(),True)])\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# VTL WindowClause\n",
    "\n",
    "windowClause ::= { data points | range } between limitClause and limitClausecount ( DS_1 over ( partition by Id_1 ) )\n",
    "\n",
    "It specifies how to apply a sliding window on the ordered Data Points. The keyword **data points** means that the sliding window includes a certain number of Data Points before and after the current Data Point in the order given by the orderClause. The keyword **range** means that the sliding windows includes all the Data Points whose values are in a certain range in respect to the value, for the current Data Point, of the Measure which the analytic is applied to.\n",
    "\n",
    "## data points example\n",
    "\n",
    "\n",
    "DS_r := first_value ( DS_1 over ( partition by Id_1, Id_2 order by Id_3 data points between 1 preceding and 1 following) )\n",
    "\n",
    "The current row index is 0. The window frame is 1 preceding, and 1 following, so the window always contains three rows with index [-1,0,1] at most. Note that the frame could not overwrite the window of the partition. For example, for the last row of one partition, you will only have two rows [-1,0], because current row is the last row, and the next row is in another partition.  For the first row of one partition it's the same, you will only have two rows [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------------+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|first_value_Me_1|\n",
      "+----+----+----+----+----+----------------+\n",
      "|   A|  YY|2000|   9|   3|               9|\n",
      "|   A|  YY|2011|   5|   4|               9|\n",
      "|   A|  YY|2022|  10|   2|               5|\n",
      "|   A|  YY|2023|   5|   7|              10|\n",
      "|   A|  XX|2000|   3|   1|               3|\n",
      "|   A|  XX|2011|   4|   9|               3|\n",
      "|   A|  XX|2022|   7|   5|               4|\n",
      "|   A|  XX|2023|   6|   8|               7|\n",
      "+----+----+----+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(\"Id_3\").rowsBetween(-1,1)\n",
    "target_col_name=\"Me_1\"\n",
    "new_col_name=f\"first_value_{target_col_name}\"\n",
    "df_collect=df.withColumn(new_col_name,first(target_col_name).over(win_name))\n",
    "df_collect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Range example\n",
    "DS_r := first_value ( DS_1 over ( partition by Id_1, Id_2 order by Id_3 range between 1 preceding and 1 following) )\n",
    "\n",
    "The range use the value of the order by column of the current row as baseline. For example, with the above query, the order by column is **Id_3**.\n",
    "As a result, the baseline column is **Id_3**. With range between 1 preceding and 1 following, we now use the value of current row of column **Id_3** to build the window. For the first row, the value of **Id_3** is 2000, so the window range is 2000-1=1999 and 2000+1=2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----------------+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|first_value_Me_1|\n",
      "+----+----+----+----+----+----------------+\n",
      "|   A|  YY|2000|   9|   3|               9|\n",
      "|   A|  YY|2011|   5|   4|               5|\n",
      "|   A|  YY|2022|  10|   2|              10|\n",
      "|   A|  YY|2023|   5|   7|              10|\n",
      "|   A|  XX|2000|   3|   1|               3|\n",
      "|   A|  XX|2011|   4|   9|               4|\n",
      "|   A|  XX|2022|   7|   5|               7|\n",
      "|   A|  XX|2023|   6|   8|               7|\n",
      "+----+----+----+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_col_names=[\"Id_1\",\"Id_2\"]\n",
    "win_name=Window.partitionBy(partition_col_names).orderBy(\"Id_3\").rangeBetween(-1,1)\n",
    "target_col_name=\"Me_1\"\n",
    "new_col_name=f\"first_value_{target_col_name}\"\n",
    "df_collect=df.withColumn(new_col_name,first(target_col_name).over(win_name))\n",
    "df_collect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## VarSamp Example\n",
    "\n",
    "pyspark.sql.functions.var_samp(col):\n",
    "\r\n",
    "Aggregate function: returns the unbiased sample variance of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|\n",
      "+----+----+----+----+----+\n",
      "|   A|  XX|2000|   3|   1|\n",
      "|   A|  XX|2001|   4|   9|\n",
      "|   A|  XX|2002|   7|   5|\n",
      "|   A|  XX|2003|   6|   8|\n",
      "|   A|  YY|2000|   9|   3|\n",
      "|   A|  YY|2001|   5|   4|\n",
      "|   A|  YY|2002|  10|   2|\n",
      "|   A|  YY|2003|   5|   7|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below calculation simulates the vtl syntax \"res := var_pop ( ds1 over ( partition by Id_1 order by Year range between 1 preceding and 1 following) );\"\n",
    "\n",
    "win_name=Window.partitionBy(\"Id_1\").orderBy(\"Id_3\").rangeBetween(-1,1)\n",
    "target_col1=\"Me_1\"\n",
    "target_col2=\"Me_2\"\n",
    "new_col_name1=f\"var_samp_{target_col1}\"\n",
    "new_col_name2=f\"var_samp_{target_col2}\"\n",
    "df_resu = df.withColumn(new_col_name1,var_samp(target_col1).over(win_name)) \\\n",
    "            .withColumn(new_col_name2,var_samp(target_col2).over(win_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+-----------------+------------------+\n",
      "|Id_1|Id_2|Id_3|Me_1|Me_2|    var_samp_Me_1|     var_samp_Me_2|\n",
      "+----+----+----+----+----+-----------------+------------------+\n",
      "|   A|  XX|2000|   3|   1|6.916666666666667|11.583333333333334|\n",
      "|   A|  YY|2000|   9|   3|6.916666666666667|11.583333333333334|\n",
      "|   A|  XX|2001|   4|   9|7.866666666666667| 8.000000000000002|\n",
      "|   A|  YY|2001|   5|   4|7.866666666666667| 8.000000000000002|\n",
      "|   A|  XX|2002|   7|   5|4.566666666666666| 6.966666666666667|\n",
      "|   A|  YY|2002|  10|   2|4.566666666666666| 6.966666666666667|\n",
      "|   A|  XX|2003|   6|   8|4.666666666666667|               7.0|\n",
      "|   A|  YY|2003|   5|   7|4.666666666666667|               7.0|\n",
      "+----+----+----+----+----+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_resu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
